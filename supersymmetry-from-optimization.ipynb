{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"supersymmetry-from-optimization.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP2VOvZTkyUn2E6jim7O1C4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1cUFjZ08zSe8","colab_type":"text"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"GzId8uPQzUHq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595713778325,"user_tz":240,"elapsed":367,"user":{"displayName":"Mathew Calkins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjylyyddgan-3LU70VDNPYNh-enpDZhUMMpWtJ0=s64","userId":"01532153278706531580"}}},"source":["import random\n","import numpy as np\n","import sys\n","np.set_printoptions(threshold=sys.maxsize)"],"execution_count":59,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"juWfvZZYy5Wa","colab_type":"text"},"source":["# Auxiliary methods"]},{"cell_type":"code","metadata":{"id":"uKME3eKky7tI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595708101691,"user_tz":240,"elapsed":360,"user":{"displayName":"Mathew Calkins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjylyyddgan-3LU70VDNPYNh-enpDZhUMMpWtJ0=s64","userId":"01532153278706531580"}}},"source":["def Delta(I,J):\n","  if (I == J):\n","    return 1\n","  return 0\n","# Functions to randomly generate initial L-matrices\n","def RandMatrix(d):\n","  output = [[random.uniform(-10,10) for i in range(d)] for j in range(d)]\n","  return output\n","def RandMatrices(N, d):\n","  return [RandMatrix(d) for I in range(N)]"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"5m7Ci3hczDkR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595708101884,"user_tz":240,"elapsed":455,"user":{"displayName":"Mathew Calkins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjylyyddgan-3LU70VDNPYNh-enpDZhUMMpWtJ0=s64","userId":"01532153278706531580"}}},"source":["def ErrorMatrices(ells, arrs, N, d):\n","  output = [[\n","             np.matmul(ells[I],arrs[J]) + np.matmul(ells[J],arrs[I]) - 2*Delta(I,J)*np.identity(d)\n","             for J in range(N)] for I in range(N)]\n","  return output"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJcLFMhuzFU8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595708101889,"user_tz":240,"elapsed":266,"user":{"displayName":"Mathew Calkins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjylyyddgan-3LU70VDNPYNh-enpDZhUMMpWtJ0=s64","userId":"01532153278706531580"}}},"source":["# Functions for calculating total error (i.e. cost function)\n","def SquaredNorm(matrix):\n","  return np.trace(np.matmul(np.transpose(matrix), matrix))\n","\n","def TotalError(ErrorMatrices, N):\n","  output = 0\n","  for I in range(N):\n","    for J in range(N):\n","      output += SquaredNorm(ErrorMatrices[I][J])\n","  return output"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"OyM6lNaWzGMA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595708102680,"user_tz":240,"elapsed":370,"user":{"displayName":"Mathew Calkins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjylyyddgan-3LU70VDNPYNh-enpDZhUMMpWtJ0=s64","userId":"01532153278706531580"}}},"source":["# Function for computing gradient w.r.t. components of L-matrices\n","def Gradient(ells, arrs, N, d):\n","  grad_L = [np.zeros((d,d)) for I in range(N)]\n","  grad_R = [np.zeros((d,d)) for I in range(N)]\n","  # Auxiliary quantity\n","  # We're repeatedly computing L_I*R_J here. Just compute it once!\n","\n","  subaux = [[np.matmul(ells[I],arrs[J]) for J in range(N)] for I in range(N)]\n","  # Further optimization: compute the UR entries first, then copy to LL\n","  aux = [[subaux[I][J] + subaux[J][I] for J in range(N)] for I in range(N)]\n","  # Add 2nd-order terms\n","  for I in range(N):\n","    for i in range(d):\n","      for j in range(d):\n","        grad_L[I][i][j] -= 8*arrs[I][j][i]\n","        grad_R[I][i][j] -= 8*ells[I][j][i]\n","        # Add 4th-order terms\n","        for Jhat in range(N):\n","          for jhat in range(d):\n","            grad_L[I][i][j] += 4*aux[I][Jhat][i][jhat]*arrs[Jhat][j][jhat]\n","            grad_R[I][i][j] += 4*aux[I][Jhat][jhat][j]*ells[Jhat][jhat][i]\n","\n","\n","  return [grad_L, grad_R]"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DND7CKjVzLgX","colab_type":"text"},"source":["# Sanity check: Gradient should vanish at known solutions"]},{"cell_type":"code","metadata":{"id":"_cAic4jizIFC","colab_type":"code","colab":{}},"source":["N = 4\n","d = 4\n","\n","# Randomize L and R matrices\n","ells = RandMatrices(N,d)\n","arrs = RandMatrices(N,d)\n","\n","# Initial L and R matrices at known solutions\n","ells[0] = [[1,0,0,0],[0,0,0,-1],[0,1,0,0],[0,0,-1,0]]\n","ells[1] = [[0,1,0,0],[0,0,1,0],[-1,0,0,0],[0,0,0,-1]]\n","ells[2] = [[0,0,1,0],[0,-1,0,0],[0,0,0,-1],[1,0,0,0]]\n","ells[3] = [[0,0,0,1],[1,0,0,0],[0,0,1,0],[0,1,0,0]]\n","ells = [ells[I] for I in range(N)]\n","for I in range(4):\n","  arrs[I] = np.transpose(ells[I])\n","\n","# Compute cost function\n","error_matrices = ErrorMatrices(ells, arrs, N, d)\n","total_error = TotalError(error_matrices, N)\n","print(\"Total error = \", total_error)\n","\n","# Compute gradient\n","[grad_L, grad_R] = Gradient(ells, arrs, N, d)\n","print(\"\\nGrad_L[0] = \\n\", grad_L[0])\n","print(\"\\nGrad_L[1] = \\n\", grad_L[1])\n","print(\"\\nGrad_L[2] = \\n\", grad_L[2])\n","print(\"\\nGrad_L[3] = \\n\", grad_L[3])\n","print(\"\\nGrad_R[0] = \\n\", grad_R[0])\n","print(\"\\nGrad_R[1] = \\n\", grad_R[1])\n","print(\"\\nGrad_R[2] = \\n\", grad_R[2])\n","print(\"\\nGrad_R[3] = \\n\", grad_R[3])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y-XO12_Szgo2","colab_type":"text"},"source":["# Given 3 known 4x4 L and R matrix pairs, solve for the last pair.\n","## Here and below, picking a step size that gives reasonably quick convergence without overstepping early on is mostly done by manual experimentation.\n","## Unlike runs where all parameters are free to vary, this restricted gradient descent sometimes settles in non-global minima. If that happens, just re-run."]},{"cell_type":"code","metadata":{"id":"RMQzyB8XzROO","colab_type":"code","colab":{}},"source":["N = 4\n","d = 4\n","\n","# Randomize L and R matrices\n","ells = RandMatrices(N,d)\n","arrs = RandMatrices(N,d)\n","\n","# Initial L and R matrices at known solutions (except the last one)\n","ells[0] = [[1.1,0.0,0.0,0.0],[0.0,0.0,0.0,-1.0],[0.0,1.0,0.0,0.0],[0.0,0.0,-1.0,0.0]]\n","ells[1] = [[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[-1.0,0.0,0.0,0.0],[0.0,0.0,0.0,-1.0]]\n","ells[2] = [[0.0,0.0,1.0,0.0],[0.0,-1.0,0.0,0.0],[0.0,0.0,0.0,-1.0],[1.0,0.0,0.0,0.0]]\n","for I in range(3):\n","  arrs[I] = np.transpose(ells[I])\n","\n","# Set learning rate\n","epsilon = .0003\n","\n","error_matrices = ErrorMatrices(ells, arrs, N, d)\n","total_error = TotalError(error_matrices, N)\n","print(\"Cost function: \", total_error)\n","\n","# Gradient descent to find the last L and R matrices.\n","for count in range(15000):\n","  [grad_L, grad_R] = Gradient(ells, arrs, N, d)\n","  for i in range(d):\n","    for j in range(d):\n","      ells[3][i][j] -= epsilon*grad_L[3][i][j]\n","      arrs[3][i][j] -= epsilon*grad_R[3][i][j]\n","\n","  error_matrices = ErrorMatrices(ells, arrs, N, d)\n","  total_error = TotalError(error_matrices, N)\n","  if (count % 250 == 0):\n","      print(\"Cost function: \", total_error)\n","\n","print(\"\\nL[3] = \\n\", ells[3])\n","print(\"\\nR[3] = \\n\", arrs[3])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FnP-W4u30rCo","colab_type":"text"},"source":["# Find 2 2x2 L and R matrices entirely from scratch.\n","## If you run this and find that it seems to get stuck, just add more steps and rerun (but of course comment out the RandMatrices() step first so the run doesn't restart). It sometimes hangs out for a while in regions of shallow gradient, but will keep descending if you wait.\n","## Here and elsewhere, the choice of step size is mostly from manual experimentation."]},{"cell_type":"code","metadata":{"id":"Wxqy5PqXzjaC","colab_type":"code","colab":{}},"source":["N = 2\n","d = 2\n","\n","# Randomize L and R matrices\n","ells = RandMatrices(N,d)\n","arrs = RandMatrices(N,d)\n","\n","# Set learning rate\n","epsilon = .001\n","\n","error_matrices = ErrorMatrices(ells, arrs, N, d)\n","total_error = TotalError(error_matrices, N)\n","print(\"Cost function: \", total_error)\n","\n","# Gradient descent to find the last L and R matrices.\n","for count in range(10000):\n","  [grad_L, grad_R] = Gradient(ells, arrs, N, d)\n","  for I in range(N):\n","    for i in range(d):\n","      for j in range(d):\n","        ells[I][i][j] -= epsilon*grad_L[I][i][j]\n","        arrs[I][i][j] -= epsilon*grad_R[I][i][j]\n","\n","  error_matrices = ErrorMatrices(ells, arrs, N, d)\n","  total_error = TotalError(error_matrices, N)\n","  if (count % 500 == 0):\n","      print(\"Cost function: \", total_error)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4x1Fnii_1oU7","colab_type":"text"},"source":["# Find 4 4x4 L and R matrices entirely from scratch."]},{"cell_type":"code","metadata":{"id":"2df0N6GQ0t4X","colab_type":"code","colab":{}},"source":["N = 4\n","d = 4\n","\n","# Randomize L and R matrices\n","ells = RandMatrices(N,d)\n","arrs = RandMatrices(N,d)\n","\n","# Set learning rate\n","epsilon = .000001\n","\n","error_matrices = ErrorMatrices(ells, arrs, N, d)\n","total_error = TotalError(error_matrices, N)\n","print(\"Cost function: \", total_error)\n","\n","# Gradient descent to find the last L and R matrices.\n","for count in range(100000):\n","  if (count == 10000):\n","    print(\"Increasing stepsize to 10e-4\")\n","    epsilon = .0001\n","  if (count == 20000):\n","    print(\"Increasing stepsize to 3x10e-3\")\n","    epsilon = .0003\n","  [grad_L, grad_R] = Gradient(ells, arrs, N, d)\n","  for I in range(N):\n","    for i in range(d):\n","      for j in range(d):\n","        ells[I][i][j] -= epsilon*grad_L[I][i][j]\n","        arrs[I][i][j] -= epsilon*grad_R[I][i][j]\n","\n","  error_matrices = ErrorMatrices(ells, arrs, N, d)\n","  total_error = TotalError(error_matrices, N)\n","  if (count % 1000 == 0):\n","      print(\"Cost function: \", total_error)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fO_STSyx9owS","colab_type":"text"},"source":["# Finding 16 128x128 L and R matrices entirely from scratch\n","## The outputs here are novel, as previously the only 1D off-shell (N,d)=(16,128) representations in the literature were those generated by dimensional reduction of higher-D representations.\n","## Given the scale, we make a few optimizations here not present for smaller representations, namely:\n","*   Computing the gradient directly rather than by calling the auxiliary function above to avoid making excess copies.\n","*   Descending as far as possible before recomputing the gradient.\n","*   Gradual simulated annealing of the step size within each cycle so that we don't waste time making excessively small steps nor risk significant overstepping.\n","\n","## In practice, we found that this took about 2500 steps and roughly 30 minutes to run, with a significant temporary slowdown around E = 7000.\n","\n"]},{"cell_type":"code","metadata":{"id":"qI9mEgbVr8No","colab_type":"code","colab":{}},"source":["N = 16\n","d = 128\n","\n","# Randomize L and R matrices\n","# ells = RandMatrices(N,d)\n","# arrs = RandMatrices(N,d)\n","\n","# Set learning rate\n","epsilon = .000001\n","error_matrices = ErrorMatrices(ells, arrs, N, d)\n","total_error = TotalError(error_matrices, N)\n","print(\"Initial error: \", total_error)\n","\n","for count in range(2500):\n","\n","  # Compute gradient\n","  grad_L = [np.zeros((d,d)) for I in range(N)]\n","  grad_R = [np.zeros((d,d)) for I in range(N)]\n","  for I in range(N):\n","    for Iprime in range(N):\n","      grad_R[I] += 4*np.matmul(np.transpose(ells[Iprime]), error_matrices[Iprime][I])\n","      grad_L[I] += 4*np.matmul(error_matrices[Iprime][I], np.transpose(arrs[Iprime]))\n","  \n","  old_cost = total_error\n","  \n","  # To avoid excessively small steps, we let the step size gradually (but\n","  # exponentially) grow inside the loop.\n","  # Assuming optimal step size is similar from cycle to cycle, we initialize it\n","  # half of an order of magnitude below the final step size from the last cycle.\n","  epsilon = epsilon / 3.0\n","\n","  print(\"Descending...\")\n","  while True:\n","    # Inch along gradient until it stops decreasing.\n","    epsilon = epsilon*1.2;\n","    old_error = total_error\n","    for I in range(N):\n","      ells[I] -= epsilon*grad_L[I]\n","      arrs[I] -= epsilon*grad_R[I]\n","    error_matrices = ErrorMatrices(ells, arrs, N, d)\n","    total_error = TotalError(error_matrices, N)\n","    if (total_error >= old_error):\n","      print(\"Cost function: \", total_error)\n","      break"],"execution_count":null,"outputs":[]}]}